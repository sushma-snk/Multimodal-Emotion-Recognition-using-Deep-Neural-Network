# Multimodal Human Emotion Recognition
Emotion recognition is vital to human-to-human communication and to accomplish a complete interaction between humans and machines. Emotions of humansâ€™ manifest in their facial expressions, voice, gestures, bodily movements, and posture. This project explores the use of deep learning techniques to address the problem of machine understanding of human affective behaviour and improve the performance of both unimodal and multimodal emotion recognition models.
We study the use of several deep learning networks like convolutional neural networks (CNNs) for this task. We also explore novel fusion techniques like Feature level fusion to capture the latent correlation and complementary information between the modalities, and thereby improving overall emotion recognition accuracy. This project also addresses the issue of lack of sufficient annotated emotion data for training since deep learning models can easily over-fit on small amounts of emotion data and do not generalize well under mismatched condition. The results achieved for the models were: 65% for FER, 72% for SER and 74% for MER.
